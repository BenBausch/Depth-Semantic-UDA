Length Target Train Loader: 44625
Length Target Validation Loader: 250
Length Source Train Loader: 44625
No checkpoint is used. Training from scratch!
[{'silog_depth': {'weight': 0.85}, 'bce': {'r': 0.3, 'ignore_index': 250}, 'snr': 'None'}]
[{'silog_depth': 1, 'bce': 0.001, 'snr': 0.01}]
Training supervised on source dataset using dense depth!
Training supervised on source dataset using semantic annotations!
Source ground truth scale is used for computing depth errors while training.
Source ground truth scale is used for computing depth errors while validating.
Training unsupervised on target dataset using self supervised depth!
Training started...
Training epoch 0 | batch 0
Batch on Device 0 computed in 3.241734504699707 seconds.
tensor([10.3966], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time for loss calculation for source: 0.037970781326293945
Time for loss calculation for target: 0.022300004959106445
Time for Loss backward: 4.87380051612854
Time needed for the batch 8.383934497833252
Time needed for logging 4.982948303222656e-05
Training epoch 0 | batch 1
Batch on Device 0 computed in 0.7188539505004883 seconds.
tensor([11.1754], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time for loss calculation for source: 0.3426847457885742
Time for loss calculation for target: 0.03707098960876465
Time for Loss backward: 1.5831613540649414
Time needed for the batch 2.6948306560516357
Time needed for logging 0.04858231544494629
Training epoch 0 | batch 2
Batch on Device 0 computed in 0.7099688053131104 seconds.
tensor([8.3659], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time for loss calculation for source: 0.341156005859375
Time for loss calculation for target: 0.011656999588012695
Time for Loss backward: 1.587059497833252
Time needed for the batch 2.6579885482788086
Time needed for logging 0.007802009582519531
Training epoch 0 | batch 3
Batch on Device 0 computed in 0.7100379467010498 seconds.
tensor([9.0052], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time for loss calculation for source: 0.34143924713134766
Time for loss calculation for target: 0.01170969009399414
Time for Loss backward: 1.5907089710235596
Time needed for the batch 2.662384510040283
Time needed for logging 0.0072116851806640625
Training epoch 0 | batch 4
Batch on Device 0 computed in 0.7152082920074463 seconds.
tensor([8.0499], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time for loss calculation for source: 0.34349894523620605
Time for loss calculation for target: 0.01169133186340332
Time for Loss backward: 1.594083309173584
Time needed for the batch 2.6739797592163086
Time needed for logging 0.007188081741333008
Training epoch 0 | batch 5
Batch on Device 0 computed in 0.715848445892334 seconds.
tensor([7.5481], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time for loss calculation for source: 0.34351301193237305
Time for loss calculation for target: 0.011596441268920898
Time for Loss backward: 1.5960404872894287
Time needed for the batch 2.6752045154571533
Time needed for logging 0.007206439971923828
Training epoch 0 | batch 6
Batch on Device 0 computed in 0.7151577472686768 seconds.
tensor([10.9939], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time for loss calculation for source: 0.34349822998046875
Time for loss calculation for target: 0.011683225631713867
Time for Loss backward: 1.597858190536499
Time needed for the batch 2.6762735843658447
Time needed for logging 0.007212162017822266
Training epoch 0 | batch 7
Batch on Device 0 computed in 0.7151577472686768 seconds.
tensor([7.9209], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time for loss calculation for source: 0.34366869926452637
Time for loss calculation for target: 0.011806011199951172
Time for Loss backward: 1.5981223583221436
Time needed for the batch 2.6769676208496094
Time needed for logging 0.007207632064819336
Training epoch 0 | batch 8
Batch on Device 0 computed in 0.7151448726654053 seconds.
tensor([10.3671], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time for loss calculation for source: 0.34345436096191406
Time for loss calculation for target: 0.011636734008789062
Time for Loss backward: 1.5964057445526123
Time needed for the batch 2.6756999492645264
Time needed for logging 0.0072324275970458984
Training epoch 0 | batch 9
Batch on Device 0 computed in 0.7153522968292236 seconds.
tensor([9.0632], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time for loss calculation for source: 0.3434913158416748
Time for loss calculation for target: 0.011629819869995117
Time for Loss backward: 1.5972940921783447
Time needed for the batch 2.6756157875061035
Time needed for logging 0.007256031036376953
Total time for 10 batches 60.765273571014404
Validation
hey
set eval mode
set eval mode
set eval mode
set eval mode
set eval mode
/home/bauschb/miniconda3/envs/Masterthesis/lib/python3.8/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  warnings.warn(
/home/bauschb/miniconda3/envs/Masterthesis/lib/python3.8/site-packages/torch/nn/functional.py:4003: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
