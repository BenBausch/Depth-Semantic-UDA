Length Target Train Loader: 44625
Length Target Validation Loader: 250
Length Source Train Loader: 44625
No checkpoint is used. Training from scratch!
[{'silog_depth': {'weight': 0.85}, 'bce': {'r': 0.3, 'ignore_index': 250}, 'snr': 'None'}]
[{'silog_depth': 1, 'bce': 0.001, 'snr': 0.01}]
Training supervised on source dataset using dense depth!
Training supervised on source dataset using semantic annotations!
Source ground truth scale is used for computing depth errors while training.
Source ground truth scale is used for computing depth errors while validating.
Training unsupervised on target dataset using self supervised depth!
Training started...
/home/bauschb/miniconda3/envs/Masterthesis/lib/python3.8/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  warnings.warn(
/home/bauschb/miniconda3/envs/Masterthesis/lib/python3.8/site-packages/torch/nn/functional.py:4003: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
Training epoch 0 | batch 0
Batch on Device 0 computed in 3.3338623046875 seconds.
tensor([10.3966], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time for loss calculation for source: 0.03903460502624512
Time for loss calculation for target: 0.02262115478515625
Time for Loss backward: 4.862544059753418
Time needed for the batch 8.474884033203125
Time needed for logging 5.1021575927734375e-05
Training epoch 0 | batch 1
Batch on Device 0 computed in 0.7108175754547119 seconds.
tensor([11.1754], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time for loss calculation for source: 0.3427886962890625
Time for loss calculation for target: 0.03480935096740723
Time for Loss backward: 1.588860034942627
Time needed for the batch 2.6921985149383545
Time needed for logging 0.05192708969116211
Training epoch 0 | batch 2
Batch on Device 0 computed in 0.7100574970245361 seconds.
tensor([8.3659], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time for loss calculation for source: 0.3412926197052002
Time for loss calculation for target: 0.01179361343383789
Time for Loss backward: 1.5869824886322021
Time needed for the batch 2.658747434616089
Time needed for logging 0.006720781326293945
Training epoch 0 | batch 3
Batch on Device 0 computed in 0.7099785804748535 seconds.
tensor([9.0052], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time for loss calculation for source: 0.3410923480987549
Time for loss calculation for target: 0.01163172721862793
Time for Loss backward: 1.5927879810333252
Time needed for the batch 2.6632256507873535
Time needed for logging 0.006674051284790039
Training epoch 0 | batch 4
Batch on Device 0 computed in 0.7153120040893555 seconds.
tensor([8.0499], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time for loss calculation for source: 0.34361910820007324
Time for loss calculation for target: 0.011765718460083008
Time for Loss backward: 1.595254898071289
Time needed for the batch 2.6739768981933594
Time needed for logging 0.006724119186401367
Training epoch 0 | batch 5
Batch on Device 0 computed in 0.7153735160827637 seconds.
tensor([7.5481], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time for loss calculation for source: 0.3434600830078125
Time for loss calculation for target: 0.011604785919189453
Time for Loss backward: 1.5882103443145752
Time needed for the batch 2.668511152267456
Time needed for logging 0.006720304489135742
Training epoch 0 | batch 6
Batch on Device 0 computed in 0.7197530269622803 seconds.
tensor([10.9939], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time for loss calculation for source: 0.3434774875640869
Time for loss calculation for target: 0.011618852615356445
Time for Loss backward: 1.5947186946868896
Time needed for the batch 2.677562713623047
Time needed for logging 0.0066928863525390625
Training epoch 0 | batch 7
Batch on Device 0 computed in 0.7152302265167236 seconds.
tensor([7.9209], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time for loss calculation for source: 0.3437683582305908
Time for loss calculation for target: 0.011898994445800781
Time for Loss backward: 1.597884178161621
Time needed for the batch 2.676804780960083
Time needed for logging 0.006720781326293945
Training epoch 0 | batch 8
Batch on Device 0 computed in 0.7151985168457031 seconds.
tensor([10.3671], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time for loss calculation for source: 0.3434276580810547
Time for loss calculation for target: 0.011679410934448242
Time for Loss backward: 1.5883686542510986
Time needed for the batch 2.6661643981933594
Time needed for logging 0.007016420364379883
Training epoch 0 | batch 9
Batch on Device 0 computed in 0.715463399887085 seconds.
tensor([9.0632], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time for loss calculation for source: 0.3438997268676758
Time for loss calculation for target: 0.011876821517944336
Time for Loss backward: 1.5942723751068115
Time needed for the batch 2.673532485961914
Time needed for logging 0.006821155548095703
Total time for 10 batches 60.54970860481262
Validation
