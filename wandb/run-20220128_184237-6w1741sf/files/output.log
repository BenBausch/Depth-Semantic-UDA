Length Target Train Loader: 44625
Length Target Validation Loader: 250
Length Source Train Loader: 44625
No checkpoint is used. Training from scratch!
[{'silog_depth': {'weight': 0.85}, 'bce': {'r': 0.3, 'ignore_index': 250}, 'snr': 'None'}]
[{'silog_depth': 1, 'bce': 0.001, 'snr': 0.01}]
Training supervised on source dataset using dense depth!
Training supervised on source dataset using semantic annotations!
Source ground truth scale is used for computing depth errors while training.
Source ground truth scale is used for computing depth errors while validating.
Training unsupervised on target dataset using self supervised depth!
Training started...
/home/bauschb/miniconda3/envs/Masterthesis/lib/python3.8/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  warnings.warn(
/home/bauschb/miniconda3/envs/Masterthesis/lib/python3.8/site-packages/torch/nn/functional.py:4003: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
Training epoch 0 | batch 0
Batch on Device 0 computed in 3.30243182182312 seconds.
tensor([10.3966], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time needed for the batch 8.44266414642334
Time for Loss backward: 4.870606184005737
Training epoch 0 | batch 1
Batch on Device 0 computed in 0.7146556377410889 seconds.
tensor([11.1754], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time needed for the batch 2.693854570388794
Time for Loss backward: 1.5876071453094482
Training epoch 0 | batch 2
Batch on Device 0 computed in 0.7101349830627441 seconds.
tensor([8.3659], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time needed for the batch 2.6573829650878906
Time for Loss backward: 1.5839965343475342
Training epoch 0 | batch 3
Batch on Device 0 computed in 0.7099885940551758 seconds.
tensor([9.0052], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time needed for the batch 2.6634163856506348
Time for Loss backward: 1.5924046039581299
Training epoch 0 | batch 4
Batch on Device 0 computed in 0.7151675224304199 seconds.
tensor([8.0499], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time needed for the batch 2.675116539001465
Time for Loss backward: 1.5963294506072998
Training epoch 0 | batch 5
Batch on Device 0 computed in 0.7151787281036377 seconds.
tensor([7.5481], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time needed for the batch 2.6763930320739746
Time for Loss backward: 1.5973520278930664
Training epoch 0 | batch 6
Batch on Device 0 computed in 0.7151615619659424 seconds.
tensor([10.9939], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time needed for the batch 2.676717758178711
Time for Loss backward: 1.5957574844360352
Training epoch 0 | batch 7
Batch on Device 0 computed in 0.7152762413024902 seconds.
tensor([7.9209], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time needed for the batch 2.6743571758270264
Time for Loss backward: 1.5922894477844238
Training epoch 0 | batch 8
Batch on Device 0 computed in 0.7158534526824951 seconds.
tensor([10.3671], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time needed for the batch 2.6739044189453125
Time for Loss backward: 1.592374324798584
Training epoch 0 | batch 9
Batch on Device 0 computed in 0.7161779403686523 seconds.
tensor([9.0632], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time needed for the batch 2.6719491481781006
Time for Loss backward: 1.592017412185669
Total time for 10 batches 60.42106008529663
Validation
set eval mode
set eval mode
set eval mode
set eval mode
