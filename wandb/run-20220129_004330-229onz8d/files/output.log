Length Target Train Loader: 44625
Length Target Validation Loader: 250
Length Source Train Loader: 44625
No checkpoint is used. Training from scratch!
[{'silog_depth': {'weight': 0.85}, 'bce': {'r': 0.3, 'ignore_index': 250}, 'snr': 'None'}]
[{'silog_depth': 1, 'bce': 0.001, 'snr': 0.01}]
Training supervised on source dataset using dense depth!
Training supervised on source dataset using semantic annotations!
Source ground truth scale is used for computing depth errors while training.
Source ground truth scale is used for computing depth errors while validating.
Training unsupervised on target dataset using self supervised depth!
Training started...
/home/bauschb/miniconda3/envs/Masterthesis/lib/python3.8/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  warnings.warn(
/home/bauschb/miniconda3/envs/Masterthesis/lib/python3.8/site-packages/torch/nn/functional.py:4003: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
WARNING:root:device cuda:0
WARNING:root:device cuda:0
WARNING:root:device cuda:0
WARNING:root:device cuda:0
WARNING:root:device cuda:0
Training epoch 0 | batch 0
tensor([10.3966], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time needed for the batch 8.690043449401855
Time for Loss backward: 5.023428916931152
Training epoch 0 | batch 1
tensor([11.1754], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time needed for the batch 2.821068525314331
Time for Loss backward: 1.6995668411254883
Training epoch 0 | batch 2
tensor([8.3659], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time needed for the batch 2.789835214614868
Time for Loss backward: 1.7006800174713135
Training epoch 0 | batch 3
tensor([9.0052], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time needed for the batch 2.8064606189727783
Time for Loss backward: 1.7081997394561768
Training epoch 0 | batch 4
tensor([8.0499], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time needed for the batch 2.800922155380249
Time for Loss backward: 1.7031874656677246
Training epoch 0 | batch 5
tensor([7.5481], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time needed for the batch 2.806441068649292
Time for Loss backward: 1.707263469696045
Training epoch 0 | batch 6
tensor([10.9939], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time needed for the batch 2.8045566082000732
Time for Loss backward: 1.706923246383667
Training epoch 0 | batch 7
tensor([7.9209], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time needed for the batch 2.8073625564575195
Time for Loss backward: 1.709953784942627
Training epoch 0 | batch 8
tensor([10.3671], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time needed for the batch 2.803806781768799
Time for Loss backward: 1.705730676651001
Training epoch 0 | batch 9
tensor([9.0632], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time needed for the batch 2.81135892868042
Time for Loss backward: 1.7131729125976562
Total time for 10 batches 60.66156983375549
