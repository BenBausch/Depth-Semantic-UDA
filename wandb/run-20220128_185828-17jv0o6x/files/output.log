Length Target Train Loader: 44625
Length Target Validation Loader: 250
Length Source Train Loader: 44625
No checkpoint is used. Training from scratch!
[{'silog_depth': {'weight': 0.85}, 'bce': {'r': 0.3, 'ignore_index': 250}, 'snr': 'None'}]
[{'silog_depth': 1, 'bce': 0.001, 'snr': 0.01}]
Training supervised on source dataset using dense depth!
Training supervised on source dataset using semantic annotations!
Source ground truth scale is used for computing depth errors while training.
Source ground truth scale is used for computing depth errors while validating.
Training unsupervised on target dataset using self supervised depth!
Training started...
/home/bauschb/miniconda3/envs/Masterthesis/lib/python3.8/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  warnings.warn(
/home/bauschb/miniconda3/envs/Masterthesis/lib/python3.8/site-packages/torch/nn/functional.py:4003: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
WARNING:root:device cuda:0
WARNING:root:device cuda:0
WARNING:root:device cuda:0
WARNING:root:device cuda:0
WARNING:root:device cuda:0
Training epoch 0 | batch 0
Batch on Device 0 computed in 3.4507694244384766 seconds.
tensor([10.3966], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time needed for the batch 8.587780237197876
Time for Loss backward: 4.870464324951172
Training epoch 0 | batch 1
Batch on Device 0 computed in 0.7100028991699219 seconds.
tensor([11.1754], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time needed for the batch 2.6907498836517334
Time for Loss backward: 1.58760666847229
Training epoch 0 | batch 2
Batch on Device 0 computed in 0.710099458694458 seconds.
tensor([8.3659], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time needed for the batch 2.6594395637512207
Time for Loss backward: 1.5872118473052979
Training epoch 0 | batch 3
Batch on Device 0 computed in 0.7103023529052734 seconds.
tensor([9.0052], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time needed for the batch 2.6615850925445557
Time for Loss backward: 1.58607816696167
Training epoch 0 | batch 4
Batch on Device 0 computed in 0.7101027965545654 seconds.
tensor([8.0499], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time needed for the batch 2.666501045227051
Time for Loss backward: 1.5923492908477783
Training epoch 0 | batch 5
Batch on Device 0 computed in 0.7152783870697021 seconds.
tensor([7.5481], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time needed for the batch 2.6787986755371094
Time for Loss backward: 1.597376823425293
Training epoch 0 | batch 6
Batch on Device 0 computed in 0.7152984142303467 seconds.
tensor([10.9939], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time needed for the batch 2.6777660846710205
Time for Loss backward: 1.5985488891601562
Training epoch 0 | batch 7
Batch on Device 0 computed in 0.7152254581451416 seconds.
tensor([7.9209], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time needed for the batch 2.675825595855713
Time for Loss backward: 1.59580659866333
Training epoch 0 | batch 8
Batch on Device 0 computed in 0.715240478515625 seconds.
tensor([10.3671], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time needed for the batch 2.6753063201904297
Time for Loss backward: 1.5958609580993652
Training epoch 0 | batch 9
Batch on Device 0 computed in 0.7151143550872803 seconds.
tensor([9.0632], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time needed for the batch 2.677030324935913
Time for Loss backward: 1.5988845825195312
Total time for 10 batches 60.658090114593506
