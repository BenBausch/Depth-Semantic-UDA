Length Target Train Loader: 44625
Length Target Validation Loader: 250
Length Source Train Loader: 44625
No checkpoint is used. Training from scratch!
[{'silog_depth': {'weight': 0.85}, 'bce': {'r': 0.3, 'ignore_index': 250}, 'snr': 'None'}]
[{'silog_depth': 1, 'bce': 0.001, 'snr': 0.01}]
Training supervised on source dataset using dense depth!
Training supervised on source dataset using semantic annotations!
Source ground truth scale is used for computing depth errors while training.
Source ground truth scale is used for computing depth errors while validating.
Training unsupervised on target dataset using self supervised depth!
Training started...
/home/bauschb/miniconda3/envs/Masterthesis/lib/python3.8/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  warnings.warn(
/home/bauschb/miniconda3/envs/Masterthesis/lib/python3.8/site-packages/torch/nn/functional.py:4003: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
  warnings.warn(
WARNING:root:device cuda:0
WARNING:root:device cuda:0
WARNING:root:device cuda:0
WARNING:root:device cuda:0
WARNING:root:device cuda:0
Training epoch 0 | batch 0
tensor([10.3966], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time needed for the batch 8.501280069351196
Time for Loss backward: 4.901692628860474
Training epoch 0 | batch 1
tensor([11.1754], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time needed for the batch 2.71095871925354
Time for Loss backward: 1.5917489528656006
Training epoch 0 | batch 2
tensor([8.3659], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time needed for the batch 2.686757802963257
Time for Loss backward: 1.6037967205047607
Training epoch 0 | batch 3
tensor([9.0052], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time needed for the batch 2.6888043880462646
Time for Loss backward: 1.5999467372894287
Training epoch 0 | batch 4
tensor([8.0499], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time needed for the batch 2.6864633560180664
Time for Loss backward: 1.5990145206451416
Training epoch 0 | batch 5
tensor([7.5481], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time needed for the batch 2.6922264099121094
Time for Loss backward: 1.6026155948638916
Training epoch 0 | batch 6
tensor([10.9939], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time needed for the batch 2.692168951034546
Time for Loss backward: 1.6050541400909424
Training epoch 0 | batch 7
tensor([7.9209], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time needed for the batch 2.6893632411956787
Time for Loss backward: 1.6025569438934326
Training epoch 0 | batch 8
tensor([10.3671], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time needed for the batch 2.6918065547943115
Time for Loss backward: 1.6040747165679932
Training epoch 0 | batch 9
tensor([9.0632], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time needed for the batch 2.6913862228393555
Time for Loss backward: 1.6026883125305176
Total time for 10 batches 60.28025794029236
