Length Target Train Loader: 44625
Length Target Validation Loader: 250
Length Source Train Loader: 44625
No checkpoint is used. Training from scratch!
[{'silog_depth': {'weight': 0.85}, 'bce': {'r': 0.3, 'ignore_index': 250}, 'snr': 'None'}]
[{'silog_depth': 1, 'bce': 0.001, 'snr': 0.01}]
Training supervised on source dataset using dense depth!
Training supervised on source dataset using semantic annotations!
Source ground truth scale is used for computing depth errors while training.
Source ground truth scale is used for computing depth errors while validating.
Training unsupervised on target dataset using self supervised depth!
Training started...
Training epoch 0 | batch 0
Batch on Device 0 computed in 3.2916576862335205 seconds.
tensor([10.3966], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time for loss calculation for source: 0.037833452224731445
Time for loss calculation for target: 0.022019147872924805
Time for Loss backward: 4.892788410186768
Time needed for the batch 8.465164184570312
Time needed for logging 5.0067901611328125e-05
Training epoch 0 | batch 1
Batch on Device 0 computed in 0.7165136337280273 seconds.
tensor([11.1754], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time for loss calculation for source: 0.34531331062316895
Time for loss calculation for target: 0.04151344299316406
Time for Loss backward: 1.5954537391662598
Time needed for the batch 2.7117512226104736
Time needed for logging 0.04748845100402832
Training epoch 0 | batch 2
Batch on Device 0 computed in 0.715287446975708 seconds.
tensor([8.3659], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time for loss calculation for source: 0.3438265323638916
Time for loss calculation for target: 0.01181793212890625
Time for Loss backward: 1.596088171005249
Time needed for the batch 2.677455425262451
Time needed for logging 0.0064508914947509766
Training epoch 0 | batch 3
Batch on Device 0 computed in 0.7155866622924805 seconds.
tensor([9.0052], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time for loss calculation for source: 0.34397459030151367
Time for loss calculation for target: 0.011811017990112305
Time for Loss backward: 1.5955805778503418
Time needed for the batch 2.6748173236846924
Time needed for logging 0.006963968276977539
Training epoch 0 | batch 4
Batch on Device 0 computed in 0.7152740955352783 seconds.
tensor([8.0499], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time for loss calculation for source: 0.3435475826263428
Time for loss calculation for target: 0.01177215576171875
Time for Loss backward: 1.5979387760162354
Time needed for the batch 2.676833391189575
Time needed for logging 0.006623029708862305
Training epoch 0 | batch 5
Batch on Device 0 computed in 0.7153339385986328 seconds.
tensor([7.5481], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time for loss calculation for source: 0.3435859680175781
Time for loss calculation for target: 0.011998653411865234
Time for Loss backward: 1.599205732345581
Time needed for the batch 2.678649663925171
Time needed for logging 0.006886720657348633
Training epoch 0 | batch 6
Batch on Device 0 computed in 0.7151951789855957 seconds.
tensor([10.9939], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time for loss calculation for source: 0.3435091972351074
Time for loss calculation for target: 0.011724233627319336
Time for Loss backward: 1.5952818393707275
Time needed for the batch 2.6736903190612793
Time needed for logging 0.0064814090728759766
Training epoch 0 | batch 7
Batch on Device 0 computed in 0.7152860164642334 seconds.
tensor([7.9209], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time for loss calculation for source: 0.3437519073486328
Time for loss calculation for target: 0.011901378631591797
Time for Loss backward: 1.5971834659576416
Time needed for the batch 2.6762757301330566
Time needed for logging 0.0064849853515625
Training epoch 0 | batch 8
Batch on Device 0 computed in 0.715280294418335 seconds.
tensor([10.3671], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time for loss calculation for source: 0.34353113174438477
Time for loss calculation for target: 0.011806011199951172
Time for Loss backward: 1.5983796119689941
Time needed for the batch 2.6764111518859863
Time needed for logging 0.007172346115112305
Training epoch 0 | batch 9
Batch on Device 0 computed in 0.7151503562927246 seconds.
tensor([9.0632], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
Time for loss calculation for source: 0.34354257583618164
Time for loss calculation for target: 0.011756420135498047
Time for Loss backward: 1.598210096359253
Time needed for the batch 2.6770401000976562
Time needed for logging 0.0069122314453125
Total time for 10 batches 60.7444269657135
Validation
/home/bauschb/miniconda3/envs/Masterthesis/lib/python3.8/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  warnings.warn(
/home/bauschb/miniconda3/envs/Masterthesis/lib/python3.8/site-packages/torch/nn/functional.py:4003: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.
